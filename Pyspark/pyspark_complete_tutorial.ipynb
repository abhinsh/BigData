{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4540d51",
   "metadata": {},
   "source": [
    "# day 1 Installation and basic operations on columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6acf41d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: 'pyspark#'\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e2d353b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2a85300",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cba5c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 20:54:57 WARN Utils: Your hostname, Gotham.local resolves to a loopback address: 127.0.0.1; using 192.168.1.75 instead (on interface en0)\n",
      "22/06/29 20:54:57 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/06/29 20:54:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('Dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f526806a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.75:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>Dataframe</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f9d168cf250>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c682558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the dataset\n",
    "df_pyspark=spark.read.option('header','true').csv('test1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d73b4d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: string (nullable = true)\n",
      " |-- experience: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#check schema\n",
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6af1c437",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.option('header','true').csv('test1.csv', inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "844dd177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6b008e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another way of reading\n",
    "df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53ac0f19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8ec949ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- name: string (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- experience: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ad2da21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_pyspark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73603040",
   "metadata": {},
   "outputs": [],
   "source": [
    "#select a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ac2cbac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.select('Name')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "19052fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|     Name|\n",
      "+---------+\n",
      "|    krish|\n",
      "|sudhanshu|\n",
      "|    sunny|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.select('Name').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b553f2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['name', 'age', 'experience']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#to see list of all columns\n",
    "\n",
    "df_pyspark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bc5b5f19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('name', 'string'), ('age', 'int'), ('experience', 'int')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check datatypes\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76c34679",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+----+-----------------+\n",
      "|summary| name| age|       experience|\n",
      "+-------+-----+----+-----------------+\n",
      "|  count|    3|   3|                3|\n",
      "|   mean| null|30.0|7.333333333333333|\n",
      "| stddev| null| 1.0|3.055050463303893|\n",
      "|    min|krish|  29|                4|\n",
      "|    max|sunny|  31|               10|\n",
      "+-------+-----+----+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "695e873d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding columns to dataframe\n",
    "df_pyspark=df_pyspark.withColumn('Experience after 2 years', df_pyspark['experience']+2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "46639cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------------------------+\n",
      "|     name|age|experience|Experience after 2 years|\n",
      "+---------+---+----------+------------------------+\n",
      "|    krish| 31|        10|                      12|\n",
      "|sudhanshu| 30|         8|                      10|\n",
      "|    sunny| 29|         4|                       6|\n",
      "+---------+---+----------+------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6556e632",
   "metadata": {},
   "outputs": [],
   "source": [
    "#dropping the columns \n",
    "df_pyspark=df_pyspark.drop('Experience after 2 years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8e06fa31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|     name|age|experience|\n",
      "+---------+---+----------+\n",
      "|    krish| 31|        10|\n",
      "|sudhanshu| 30|         8|\n",
      "|    sunny| 29|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0f11e03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "234de092",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=df_pyspark.withColumnRenamed('Name','NewName')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8ca73276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+\n",
      "|  NewName|age|experience|\n",
      "+---------+---+----------+\n",
      "|    krish| 31|        10|\n",
      "|sudhanshu| 30|         8|\n",
      "|    sunny| 29|         4|\n",
      "+---------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d85446",
   "metadata": {},
   "source": [
    "# day2 Handle missing value by mean, median etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e02edac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test2.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4bab6ed0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75927fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#drop null values \n",
    "\n",
    "df_pyspark.na.drop().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "59a8e39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in above cell all the rows which has atleast one null column gets deleted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "decc147d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#understanding features of drop() : how, thresh and subset\n",
    "df_pyspark.na.drop(how='all').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b3f8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in above cell, no rows where dropped because when we select how=all, it looks for rows which have all fields null. By default how is any\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "97d2f8ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#threshold\n",
    "df_pyspark.na.drop(thresh=2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f5d9dd71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# at least 2 non-null value should be present. i.e the rows having more than two non-null values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "2a9259fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|     null| 34|        10| 38000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#subset\n",
    "df_pyspark.na.drop(subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0816ff91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wherever the column Experience was null, the corresponding row is dropped.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e20efd",
   "metadata": {},
   "source": [
    "### fill missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ba0a46e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# lets start with the original dataset first\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e5c24f6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|      null| 40000|\n",
      "|  Missing|  34|        10| 38000|\n",
      "|  Missing|  36|      null|  null|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill has two features: value and subset\n",
    "#lets look into value \n",
    "\n",
    "df_pyspark.na.fill('Missing').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb8ffdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this filled 'Missing' wherever the field of type string is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "8eefcc9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "|   Mahesh|999|       999| 40000|\n",
      "|     null| 34|        10| 38000|\n",
      "|     null| 36|       999|   999|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.na.fill(999).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b3a47ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#similarly the above cell filled 999 wherever the field of type integer is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "46bf0e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we want to fill all the cells with either 'Missing' or 999, we have to load the dataset with feature inferSchema=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "85ba4874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+\n",
      "|     Name| age|Experience|Salary|\n",
      "+---------+----+----------+------+\n",
      "|    Krish|  31|        10| 30000|\n",
      "|Sudhanshu|  30|         8| 25000|\n",
      "|    Sunny|  29|         4| 20000|\n",
      "|     Paul|  24|         3| 20000|\n",
      "|   Harsha|  21|         1| 15000|\n",
      "|  Shubham|  23|         2| 18000|\n",
      "|   Mahesh|null|       999| 40000|\n",
      "|     null|  34|        10| 38000|\n",
      "|     null|  36|       999|   999|\n",
      "+---------+----+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filling missing value only on specific columns\n",
    "df_pyspark.na.fill(999,['Experience','Salary']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b608df2f",
   "metadata": {},
   "source": [
    "### filling with mean "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "850ed7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "fe396473",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer= Imputer(\n",
    "inputCols=['age','Experience','Salary'],outputCols=[x+'_imp' for x in ['age','Experience','Salary']]).setStrategy('mean')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "c0362c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|     Name| age|Experience|Salary|age_imp|Experience_imp|Salary_imp|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "|    Krish|  31|        10| 30000|     31|            10|     30000|\n",
      "|Sudhanshu|  30|         8| 25000|     30|             8|     25000|\n",
      "|    Sunny|  29|         4| 20000|     29|             4|     20000|\n",
      "|     Paul|  24|         3| 20000|     24|             3|     20000|\n",
      "|   Harsha|  21|         1| 15000|     21|             1|     15000|\n",
      "|  Shubham|  23|         2| 18000|     23|             2|     18000|\n",
      "|   Mahesh|null|      null| 40000|     28|             5|     40000|\n",
      "|     null|  34|        10| 38000|     34|            10|     38000|\n",
      "|     null|  36|      null|  null|     36|             5|     25750|\n",
      "+---------+----+----------+------+-------+--------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark).transform(df_pyspark).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef570c03",
   "metadata": {},
   "source": [
    "# day 3 Pyspark Dataframes: Filter, Aggregation Operations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f1fc32ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "68772d31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 21:25:18 WARN Utils: Your hostname, Gotham.local resolves to a loopback address: 127.0.0.1; using 192.168.1.75 instead (on interface en0)\n",
      "22/07/04 21:25:18 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 21:25:19 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark=SparkSession.builder.appName('dataframe').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b88660d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test1.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e3108f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "|    Sunny| 29|         4| 20000|\n",
      "|     Paul| 24|         3| 20000|\n",
      "|   Harsha| 21|         1| 15000|\n",
      "|  Shubham| 23|         2| 18000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0213a5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+------+\n",
      "|   Name|age|Experience|Salary|\n",
      "+-------+---+----------+------+\n",
      "|  Sunny| 29|         4| 20000|\n",
      "|   Paul| 24|         3| 20000|\n",
      "| Harsha| 21|         1| 15000|\n",
      "|Shubham| 23|         2| 18000|\n",
      "+-------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering those who have salary less than or equal to 20K\n",
    "df_pyspark.filter(\"Salary<=20000\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2efaee7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   Name|Experience|\n",
      "+-------+----------+\n",
      "|  Sunny|         4|\n",
      "|   Paul|         3|\n",
      "| Harsha|         1|\n",
      "|Shubham|         2|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# picking just Name and Experience of those who have salary less than or equal to 20K \n",
    "df_pyspark.filter(\"Salary<=20000\").select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8aa5c4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+----------+------+\n",
      "| Name|age|Experience|Salary|\n",
      "+-----+---+----------+------+\n",
      "|Sunny| 29|         4| 20000|\n",
      "| Paul| 24|         3| 20000|\n",
      "+-----+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# filtering based on multiple conditions\n",
    "df_pyspark.filter((df_pyspark[\"Salary\"]<=20000) & (df_pyspark[\"Experience\"]>2)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0ea5ff6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---+----------+------+\n",
      "|     Name|age|Experience|Salary|\n",
      "+---------+---+----------+------+\n",
      "|    Krish| 31|        10| 30000|\n",
      "|Sudhanshu| 30|         8| 25000|\n",
      "+---------+---+----------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# using negation (~) in filter condition\n",
    "df_pyspark.filter(~(df_pyspark[\"Salary\"]<=20000) & (df_pyspark[\"Experience\"]>2)).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce56401",
   "metadata": {},
   "source": [
    "# Groupby and Aggregation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "49506db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark=spark.read.csv('test3.csv',header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d9a0ea80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------+------+\n",
      "|     Name| Departments|salary|\n",
      "+---------+------------+------+\n",
      "|    Krish|Data Science| 10000|\n",
      "|    Krish|         IOT|  5000|\n",
      "|   Mahesh|    Big Data|  4000|\n",
      "|    Krish|    Big Data|  4000|\n",
      "|   Mahesh|Data Science|  3000|\n",
      "|Sudhanshu|Data Science| 20000|\n",
      "|Sudhanshu|         IOT| 10000|\n",
      "|Sudhanshu|    Big Data|  5000|\n",
      "|    Sunny|Data Science| 10000|\n",
      "|    Sunny|    Big Data|  2000|\n",
      "+---------+------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ac95e9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Departments: string (nullable = true)\n",
      " |-- salary: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5afa3c",
   "metadata": {},
   "source": [
    "### Groupby"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "47f93cf7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, sum(salary): bigint]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0be52c45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+\n",
      "|     Name|sum(salary)|\n",
      "+---------+-----------+\n",
      "|Sudhanshu|      35000|\n",
      "|    Sunny|      12000|\n",
      "|    Krish|      19000|\n",
      "|   Mahesh|       7000|\n",
      "+---------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Name').sum().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8f5dfe52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+\n",
      "| Departments|avg(salary)|\n",
      "+------------+-----------+\n",
      "|         IOT|     7500.0|\n",
      "|    Big Data|     3750.0|\n",
      "|Data Science|    10750.0|\n",
      "+------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').mean().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "293dee95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "| Departments|count|\n",
      "+------------+-----+\n",
      "|         IOT|    2|\n",
      "|    Big Data|    4|\n",
      "|Data Science|    4|\n",
      "+------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.groupBy('Departments').count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec9a266",
   "metadata": {},
   "source": [
    "# Linear Regression Basic Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "dff79384",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName(\"lin_reg\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "8e28534b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_reg=spark.read.csv('tips.csv',header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "84ae2f3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|\n",
      "+----------+----+------+------+---+------+----+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|\n",
      "+----------+----+------+------+---+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_reg.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "bcc2603b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- total_bill: double (nullable = true)\n",
      " |-- tip: double (nullable = true)\n",
      " |-- sex: string (nullable = true)\n",
      " |-- smoker: string (nullable = true)\n",
      " |-- day: string (nullable = true)\n",
      " |-- time: string (nullable = true)\n",
      " |-- size: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_reg.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "8be3242b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we dont need to fill. issing value here. Lets directly jump to Linear Regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "932c4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#in order to apply linear regeression, we must select dependent and independent feature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6f4ea8ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here \"total_bill\" is our target(dependent) variable and rest are independent variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "35d1b4f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets convert the strings features to numerical features wherever possible"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356172be",
   "metadata": {},
   "source": [
    "### impute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "16cfe480",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "35f56ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sex , smoker , day , time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2147ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indexer=StringIndexer(inputCols=['sex','smoker','day','time'], outputCols=[i+'_transform' for i in (['sex','smoker','day','time'])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "e92eb5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_transform|smoker_transform|day_transform|time_transform|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|          0.0|             0.0|          0.0|           0.0|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_reg_indexed=indexer.fit(df_pyspark_reg).transform(df_pyspark_reg).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "365b9e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from here, we can drop the unnecessary columns but we will simoly use something Interesting such as vector assembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "21318cf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "b1ff300b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_pyspark_reg.select(df_pyspark_reg.columns[1]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "76fb2d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "features=VectorAssembler(inputCols=['tip','size','sex_transform','smoker_transform','day_transform','time_transform'], outputCol='Vector_Features')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9637012c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "fbdd2407",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute '_jdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Input \u001b[0;32mIn [153]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m output\u001b[38;5;241m=\u001b[39m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_pyspark_reg_indexed\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/base.py:262\u001b[0m, in \u001b[0;36mTransformer.transform\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    260\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy(params)\u001b[38;5;241m.\u001b[39m_transform(dataset)\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 262\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mtype\u001b[39m(params))\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.9/site-packages/pyspark/ml/wrapper.py:396\u001b[0m, in \u001b[0;36mJavaTransformer._transform\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[0;32m--> 396\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj\u001b[38;5;241m.\u001b[39mtransform(\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m), dataset\u001b[38;5;241m.\u001b[39msparkSession)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute '_jdf'"
     ]
    }
   ],
   "source": [
    "output=features.transform(df_pyspark_reg_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "f6acdb8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This 'NoneType' object has no attribute '_jdf' error is because somehwere in assign line of code we must have used show()\n",
    "\n",
    "# Just remove show and it will work fine. I have the same issue in line 148. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "906c7848",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark_reg_indexed=indexer.fit(df_pyspark_reg).transform(df_pyspark_reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "c25eeb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_transform|smoker_transform|day_transform|time_transform|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|          0.0|             0.0|          0.0|           0.0|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark_reg_indexed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2695a8c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now i can use VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "867406b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output=features.transform(df_pyspark_reg_indexed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "4d038908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+--------------------+\n",
      "|total_bill| tip|   sex|smoker|day|  time|size|sex_transform|smoker_transform|day_transform|time_transform|     Vector_Features|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+--------------------+\n",
      "|     16.99|1.01|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|[1.01,2.0,1.0,0.0...|\n",
      "|     10.34|1.66|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|[1.66,3.0,0.0,0.0...|\n",
      "|     21.01| 3.5|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|[3.5,3.0,0.0,0.0,...|\n",
      "|     23.68|3.31|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[3.31,2.0,0.0,0.0...|\n",
      "|     24.59|3.61|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|[3.61,4.0,1.0,0.0...|\n",
      "|     25.29|4.71|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|[4.71,4.0,0.0,0.0...|\n",
      "|      8.77| 2.0|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[2.0,2.0,0.0,0.0,...|\n",
      "|     26.88|3.12|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|[3.12,4.0,0.0,0.0...|\n",
      "|     15.04|1.96|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[1.96,2.0,0.0,0.0...|\n",
      "|     14.78|3.23|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[3.23,2.0,0.0,0.0...|\n",
      "|     10.27|1.71|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[1.71,2.0,0.0,0.0...|\n",
      "|     35.26| 5.0|Female|    No|Sun|Dinner|   4|          1.0|             0.0|          1.0|           0.0|[5.0,4.0,1.0,0.0,...|\n",
      "|     15.42|1.57|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[1.57,2.0,0.0,0.0...|\n",
      "|     18.43| 3.0|  Male|    No|Sun|Dinner|   4|          0.0|             0.0|          1.0|           0.0|[3.0,4.0,0.0,0.0,...|\n",
      "|     14.83|3.02|Female|    No|Sun|Dinner|   2|          1.0|             0.0|          1.0|           0.0|[3.02,2.0,1.0,0.0...|\n",
      "|     21.58|3.92|  Male|    No|Sun|Dinner|   2|          0.0|             0.0|          1.0|           0.0|[3.92,2.0,0.0,0.0...|\n",
      "|     10.33|1.67|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|[1.67,3.0,1.0,0.0...|\n",
      "|     16.29|3.71|  Male|    No|Sun|Dinner|   3|          0.0|             0.0|          1.0|           0.0|[3.71,3.0,0.0,0.0...|\n",
      "|     16.97| 3.5|Female|    No|Sun|Dinner|   3|          1.0|             0.0|          1.0|           0.0|[3.5,3.0,1.0,0.0,...|\n",
      "|     20.65|3.35|  Male|    No|Sat|Dinner|   3|          0.0|             0.0|          0.0|           0.0|(6,[0,1],[3.35,3.0])|\n",
      "+----------+----+------+------+---+------+----+-------------+----------------+-------------+--------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "5acac25a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     Vector_Features|\n",
      "+--------------------+\n",
      "|[1.01,2.0,1.0,0.0...|\n",
      "|[1.66,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,0.0,0.0,...|\n",
      "|[3.31,2.0,0.0,0.0...|\n",
      "|[3.61,4.0,1.0,0.0...|\n",
      "|[4.71,4.0,0.0,0.0...|\n",
      "|[2.0,2.0,0.0,0.0,...|\n",
      "|[3.12,4.0,0.0,0.0...|\n",
      "|[1.96,2.0,0.0,0.0...|\n",
      "|[3.23,2.0,0.0,0.0...|\n",
      "|[1.71,2.0,0.0,0.0...|\n",
      "|[5.0,4.0,1.0,0.0,...|\n",
      "|[1.57,2.0,0.0,0.0...|\n",
      "|[3.0,4.0,0.0,0.0,...|\n",
      "|[3.02,2.0,1.0,0.0...|\n",
      "|[3.92,2.0,0.0,0.0...|\n",
      "|[1.67,3.0,1.0,0.0...|\n",
      "|[3.71,3.0,0.0,0.0...|\n",
      "|[3.5,3.0,1.0,0.0,...|\n",
      "|(6,[0,1],[3.35,3.0])|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "output.select('Vector_Features').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "7bb407a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "81188fde",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset=output.select('Vector_Features','total_bill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "94538440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|     Vector_Features|total_bill|\n",
      "+--------------------+----------+\n",
      "|[1.01,2.0,1.0,0.0...|     16.99|\n",
      "|[1.66,3.0,0.0,0.0...|     10.34|\n",
      "|[3.5,3.0,0.0,0.0,...|     21.01|\n",
      "|[3.31,2.0,0.0,0.0...|     23.68|\n",
      "|[3.61,4.0,1.0,0.0...|     24.59|\n",
      "|[4.71,4.0,0.0,0.0...|     25.29|\n",
      "|[2.0,2.0,0.0,0.0,...|      8.77|\n",
      "|[3.12,4.0,0.0,0.0...|     26.88|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|\n",
      "|[3.23,2.0,0.0,0.0...|     14.78|\n",
      "|[1.71,2.0,0.0,0.0...|     10.27|\n",
      "|[5.0,4.0,1.0,0.0,...|     35.26|\n",
      "|[1.57,2.0,0.0,0.0...|     15.42|\n",
      "|[3.0,4.0,0.0,0.0,...|     18.43|\n",
      "|[3.02,2.0,1.0,0.0...|     14.83|\n",
      "|[3.92,2.0,0.0,0.0...|     21.58|\n",
      "|[1.67,3.0,1.0,0.0...|     10.33|\n",
      "|[3.71,3.0,0.0,0.0...|     16.29|\n",
      "|[3.5,3.0,1.0,0.0,...|     16.97|\n",
      "|(6,[0,1],[3.35,3.0])|     20.65|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_dataset.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eeb57adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  applying linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "e7ffaae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "70039cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data=final_dataset.randomSplit([0.75,0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "4415fdde",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression(featuresCol='Vector_Features',labelCol='total_bill')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "a8cbce94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/07/04 23:36:32 WARN Instrumentation: [9b9e2314] regParam is zero, which might cause numerical instability and overfitting.\n"
     ]
    }
   ],
   "source": [
    "x=lr.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "45d3a439",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DenseVector([3.0826, 3.8006, -1.0978, 2.1781, -0.5032, 0.2479])"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "419da8f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7990600360466894"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.intercept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "b06dc1b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "ea2182dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=x.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "c8bee871",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+------------------+\n",
      "|     Vector_Features|total_bill|        prediction|\n",
      "+--------------------+----------+------------------+\n",
      "|(6,[0,1],[1.25,2.0])|     10.51| 12.25350913025802|\n",
      "|(6,[0,1],[1.45,2.0])|      9.55|12.870029620801512|\n",
      "|(6,[0,1],[1.47,2.0])|     10.77|12.931681669855863|\n",
      "|(6,[0,1],[1.97,2.0])|     12.02|14.472982896214592|\n",
      "| (6,[0,1],[2.0,2.0])|     13.37|14.565460969796117|\n",
      "|(6,[0,1],[2.01,2.0])|     20.23| 14.59628699432329|\n",
      "| (6,[0,1],[2.5,4.0])|     18.35| 23.70795822446935|\n",
      "| (6,[0,1],[3.0,4.0])|     20.45| 25.24925945082808|\n",
      "|(6,[0,1],[3.15,3.0])|     20.08| 21.91105180457845|\n",
      "|(6,[0,1],[3.27,2.0])|     17.78|18.480366084747292|\n",
      "| (6,[0,1],[3.6,3.0])|     24.06|23.298222908301305|\n",
      "|(6,[0,1],[5.92,3.0])|     29.03|30.449860598605817|\n",
      "|(6,[0,1],[6.73,4.0])|     48.27|36.747366599464215|\n",
      "| (6,[0,1],[9.0,4.0])|     48.33| 43.74487416713285|\n",
      "|[1.5,2.0,0.0,0.0,...|     19.08|12.265575816245715|\n",
      "|[1.5,2.0,0.0,1.0,...|     11.59|15.202301500761305|\n",
      "|[1.5,2.0,0.0,1.0,...|     12.03|13.692583243021465|\n",
      "|[1.76,2.0,0.0,1.0...|     11.24|16.003778138467844|\n",
      "|[1.8,2.0,1.0,0.0,...|     12.43|12.092553544589771|\n",
      "|[1.96,2.0,0.0,0.0...|     15.04|13.938917452440805|\n",
      "+--------------------+----------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.predictions.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "591a534f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.5229535456102288, 4.386360919202124, 35.3419878455901)"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prediction metrics\n",
    "pred.r2, pred.meanAbsoluteError, pred.meanSquaredError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0e2518",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
